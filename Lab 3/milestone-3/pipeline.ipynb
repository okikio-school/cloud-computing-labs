{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "import glob\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set a dedicated cache directory to avoid conflicts on Dataflow workers.\n",
    "torch_cache_dir = './torch_cache'\n",
    "# os.makedirs(torch_cache_dir, exist_ok=True)\n",
    "# os.environ['TORCH_HOME'] = torch_cache_dir\n",
    "\n",
    "# Alternatively, you can set the cache directory using torch.hub.set_dir:\n",
    "torch.hub.set_dir(torch_cache_dir)\n",
    "\n",
    "# ---------------------------\n",
    "# Model Initialization\n",
    "# ---------------------------\n",
    "# Load YOLO model (ensure your checkpoint path is correct)\n",
    "model = YOLO(\"checkpoints/yolov8n.pt\") # \"checkpoints/yolo11n.pt\"\n",
    "\n",
    "# Set up MiDaS model\n",
    "model_type = \"MiDaS_small\"  # Adjust model type as needed (e.g., \"DPT_Large\" for higher accuracy)\n",
    "midas = torch.hub.load(\"intel-isl/MiDaS\", model_type, trust_repo=True)\n",
    "\n",
    "# Move model to GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"PyTourch using CUDA\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"PyTourch using CPU\")\n",
    "\n",
    "midas.to(device)\n",
    "midas.eval()\n",
    "\n",
    "# Load MiDaS transforms\n",
    "midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n",
    "if model_type in [\"DPT_Large\", \"DPT_Hybrid\"]:\n",
    "    transform = midas_transforms.dpt_transform\n",
    "else:\n",
    "    transform = midas_transforms.small_transform\n",
    "\n",
    "# ---------------------------\n",
    "# Process Images from Glob\n",
    "# ---------------------------\n",
    "# Use glob to obtain a list of image paths (update the pattern as needed)\n",
    "img_paths = glob.glob(\"./datasets/[AC]_*.*\")\n",
    "# img_paths = glob.glob(\"/datasets/A_004.png\")\n",
    "\n",
    "if not img_paths:\n",
    "    print(\"No images found with the given glob pattern.\")\n",
    "\n",
    "# Loop over each image\n",
    "for img_path in img_paths:\n",
    "    print(f\"\\nProcessing image: {img_path}\")\n",
    "    \n",
    "    # Load image using PIL and convert to a NumPy array (RGB)\n",
    "    image_pil = Image.open(img_path).convert(\"RGB\")\n",
    "    image_np = np.array(image_pil)\n",
    "    \n",
    "    # ---------------------------\n",
    "    # YOLO Object Detection\n",
    "    # ---------------------------\n",
    "    # Run YOLO detection on the image (stream mode returns a list of results)\n",
    "    results = model(\n",
    "        img_path, \n",
    "        stream=True, \n",
    "        imgsz=1280,   # Increase resolution to help detect small/distant objects\n",
    "        conf=0.25     # Lower confidence threshold to catch smaller or partially occluded persons\n",
    "    )\n",
    "    \n",
    "    # Filter detections to pedestrians only (assuming class index 0 represents a person)\n",
    "    person_boxes = []\n",
    "    for result in results:\n",
    "        # print(\"=== YOLO Detection Result ===\")\n",
    "        # print(\"Boxes:\", result.boxes)\n",
    "        # print(\"Masks:\", result.masks)\n",
    "        # print(\"Keypoints:\", result.keypoints)\n",
    "        # print(\"Probabilities:\", result.probs)\n",
    "        # print(\"OBB:\", result.obb)\n",
    "        \n",
    "        for box in result.boxes:\n",
    "            if int(box.cls[0]) == 0:\n",
    "                person_boxes.append(box)\n",
    "    \n",
    "    # print(\"Filtered Person Boxes:\", person_boxes)\n",
    "    \n",
    "    # ---------------------------\n",
    "    # MiDaS Full Image Depth Estimation\n",
    "    # ---------------------------\n",
    "    # Prepare the full image for depth estimation\n",
    "    input_batch = transform(image_np).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        prediction = midas(input_batch)\n",
    "        # Resize the prediction to the original image resolution\n",
    "        prediction = torch.nn.functional.interpolate(\n",
    "            prediction.unsqueeze(1),\n",
    "            size=image_np.shape[:2],\n",
    "            mode=\"bicubic\",\n",
    "            align_corners=False,\n",
    "        ).squeeze()\n",
    "    depth_map = prediction.cpu().numpy()\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Compute and Print Pedestrian Depth Information\n",
    "    # ---------------------------\n",
    "    print(\"\\n--- Pedestrian Depth Estimates ---\")\n",
    "    for idx, box in enumerate(person_boxes):\n",
    "        # Get bounding box coordinates\n",
    "        coords = box.xyxy.cpu().numpy()[0]\n",
    "        x1, y1, x2, y2 = map(int, coords)\n",
    "        x1 = max(0, x1)\n",
    "        y1 = max(0, y1)\n",
    "        x2 = min(image_np.shape[1], x2)\n",
    "        y2 = min(image_np.shape[0], y2)\n",
    "        \n",
    "        # Extract the corresponding region from the depth map and compute average depth\n",
    "        depth_crop = depth_map[y1:y2, x1:x2]\n",
    "        avg_depth = np.mean(depth_crop)\n",
    "        \n",
    "        print(f\"Pedestrian {idx+1}: Bounding Box [x1: {x1}, y1: {y1}, x2: {x2}, y2: {y2}], Average Depth: {avg_depth:.4f}\")\n",
    "    \n",
    "    # ---------------------------\n",
    "    # Optional Visualization\n",
    "    # ---------------------------\n",
    "    # Normalize the depth map for visualization\n",
    "    depth_min, depth_max = depth_map.min(), depth_map.max()\n",
    "    depth_norm = (depth_map - depth_min) / (depth_max - depth_min + 1e-6)\n",
    "    depth_vis = (depth_norm * 255).astype(np.uint8)\n",
    "    depth_vis = cv2.applyColorMap(depth_vis, cv2.COLORMAP_PLASMA)\n",
    "    \n",
    "    # Overlay YOLO person bounding boxes on the depth map visualization\n",
    "    for box in person_boxes:\n",
    "        coords = box.xyxy.cpu().numpy()[0]\n",
    "        x1, y1, x2, y2 = map(int, coords)\n",
    "        cv2.rectangle(depth_vis, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(cv2.cvtColor(depth_vis, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(f\"Depth Map with Pedestrian Bounding Boxes: {img_path}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Add the deadsnakes PPA\n",
    "sudo add-apt-repository ppa:deadsnakes/ppa\n",
    "sudo apt update\n",
    "\n",
    "# Now try installing Python 3.11\n",
    "sudo apt install python3.11 python3.11-venv python3.11-dev\n",
    "\n",
    "# Create a Python 3.11 virtual environment and activate it\n",
    "python3.11 -m venv beam_env\n",
    "source beam_env/bin/activate\n",
    "\n",
    "# Install Apache Beam\n",
    "pip install apache-beam[gcp]==2.63.0\n",
    "\n",
    "# Check Python Version\n",
    "python3 --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "docker build . --no-cache --tag northamerica-northeast2-docker.pkg.dev/cloud-computing-449706/cloud-computing/dataflow/m3:latest\n",
    "docker push northamerica-northeast2-docker.pkg.dev/cloud-computing-449706/cloud-computing/dataflow/m3:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "export CLOUDSDK_PYTHON=python3.11\n",
    "export PYTHON_VERSION=3.11\n",
    "export BEAM_VERSION=2.63.0\n",
    "\n",
    "PROJECT=$(gcloud config list project --format \"value(core.project)\")\n",
    "BUCKET=gs://$PROJECT-bucket\n",
    "IMAGE_URI=northamerica-northeast2-docker.pkg.dev/cloud-computing-449706/cloud-computing/dataflow/m3:latest\n",
    "python src/dataflow.py   \\\n",
    "    --runner DataflowRunner   \\\n",
    "    --sdk_container_image=$IMAGE_URI   \\\n",
    "    --sdk_location=container   \\\n",
    "    --project $PROJECT   \\\n",
    "    --staging_location $BUCKET/staging   \\\n",
    "    --temp_location $BUCKET/temp   \\\n",
    "    --setup_file ./setup.py   \\\n",
    "    --input_topic projects/$PROJECT/topics/m3-image    \\\n",
    "    --output_topic projects/$PROJECT/topics/m3-output    \\\n",
    "    --region  northamerica-northeast2    \\\n",
    "    --experiment use_unsupported_python_version    \\\n",
    "    --experiments=use_runner_v2    \\\n",
    "    --streaming"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
